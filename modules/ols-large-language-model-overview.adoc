// This module is used in the following assemblies:

// * about/ols-about-openshift-lightspeed.adoc

:_mod-docs-content-type: CONCEPT
[id="ols-large-language-model-overview"]
= Large Language Model (LLM) overview 
:context: ols-large-language-model-overview

A large language model (LLM) is a type of machine learning model that can interpret and generate human-like language. When an LLM is used with a virtual assistant the LLM can interpret questions accurately and provide helpful answers in a conversational manner.

As part of the {ols-release} release, {ols-long} can rely on the following Software as a Service (SaaS) LLM providers: 

* OpenAI

* {azure-openai}

* {watsonx}

[NOTE]
====
Many self-hosted or self-managed model servers claim API compatibility with OpenAI. It is possible to configure the {ols-long} OpenAI provider to point to an API-compatible model server. If the model server is truly API-compatible, especially with respect to authentication, then it may work. These configurations have not been tested by Red Hat, and issues related to their use are outside the scope of {ols-release} support.
====

For {ols-long} configurations with {rhoai} or {rhelai}, you must host your own LLM provider rather than use a SaaS LLM provider.

[id="ibm-watsonx_{context}"]
== {watsonx}

To use {watsonx} with {ols-official}, you need an account with link:https://www.ibm.com/products/watsonx-ai[IBM Cloud's watsonx].

[id="open-ai_{context}"]
== Open AI

To use {openai} with {ols-official}, you need access to the {openai} link:https://openai.com/api/[API platform].

[id="azure-open-ai_{context}"]
== {azure-openai}

To use {azure-official} with {ols-official}, you need access to {azure-openai}.

[id="rhelai_{context}"]
== {rhelai} 

{rhelai} is OpenAI API-compatible, and is configured in a similar manner as the OpenAI provider. 

You can configure {rhelai} as the (Large Language Model) LLM provider. 

Because the {rhel} is in a different environment than the {ols-long} deployment, the model deployment must allow access using a secure connection. For more information, see link:https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_ai/1.2/html-single/building_your_rhel_ai_environment/index#creating_secure_endpoint[Optional: Allowing access to a model from a secure endpoint].


[id="rhoai_{context}"]
== {rhoai}

{rhoai} is OpenAI API-compatible, and is configured largely the same as the OpenAI provider. 

You need a Large Language Model (LLM) deployed on the single model-serving platform of {rhoai} using the Virtual Large Language Model (vLLM) runtime. If the model deployment is in a different {ocp-short-name} environment than the {ols-long} deployment, the model deployment must include a route to expose it outside the cluster. For more information, see link:https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2-latest/html/serving_models/serving-large-models_serving-large-models#about-the-single-model-serving-platform_serving-large-models[About the single-model serving platform].
