:_mod-docs-content-type: CONCEPT
[id="ols-large-language-model-configuration-overview_{context}"]

= Large Language Model (LLM) configuration overview

You can configure {rhelai} or {rhoai} as large language model (LLM) provider for the {ols-long} Service. Either of those LLM providers can use a server or inference service that processes inference queries. Configure the LLM provider before you install the {ols-long} Operator. 

Alternatively, you can connect the {ols-long} Service to one of the publicly available LLM providers, such as {watsonx}, {openai}, or {azure-openai}.

[id="rhelai-with-ols_{context}"]
== {rhelai} with {ols-long}

You can use {rhelai} to host an LLM. 

For more information, see link:https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_ai/1.4/html/generating_a_custom_llm_using_rhel_ai/index[Generating a custom LLM using RHEL AI].

[id="rhoai-with-ols_{context}"]
== {rhoai} with {ols-long}

You can use {rhoai} to host an LLM. 

For more information, see link:https://docs.redhat.com/en/documentation/red_hat_openshift_ai_cloud_service/1/html/serving_models/about-model-serving_about-model-serving#single_model_serving_platform[Single-model serving platform].

[id="watsnx-with-ols_{context}"]
== {watsonx} with {ols-long}

To configure {watsonx} as the LLM provider, you need an IBM Cloud project with access to {watsonx}. You also need your {watsonx} API key.

For more information, see the official {watsonx} link:https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/welcome-main.html?context=wx&audience=wdp[product documentation].

[id="openai-with-ols_{context}"]
== {openai} with {ols-long}

To configure {openai} as the LLM provider with {ols-long}, you need either the {openai} API key or the {openai} project name during the configuration process.

The {openai} Service has a feature for projects and service accounts. You can use a service account in a dedicated project so that you can precisely track {ols-long} usage.

For more information, see the official {openai} link:https://platform.openai.com/docs/overview[product documentation].

[id="azure-openai-with-ols_{context}"]
== {azure-openai} with {ols-long}

To configure {azure-openai} as the LLM provider, you need a {azure-openai} Service instance. You must have at least one model deployment in {azure-studio} for that instance.

For more information, see the official {azure-openai} link:https://learn.microsoft.com/en-us/azure/ai-services/openai/[product documentation].
